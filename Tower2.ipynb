{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f2fdfc-310c-41a6-8d40-5da2e20472e2",
   "metadata": {},
   "source": [
    "# Step 1: Install PyTorch and torchvision with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e605745-7661-4c1c-8fad-0649af577e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118 --user\n",
    "#!pip install matplotlib\n",
    "#!pip install pycocotools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d579e-92ff-4609-9e1d-7498861fab53",
   "metadata": {},
   "source": [
    "# Step 2: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddf0d9-478b-4b23-8fa0-f693b48903b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lib.TowerDataset import TowerDataset\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be785e-8750-4e2d-85d1-15d1d889f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print PyTorch version\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)\n",
    "\n",
    "# Print CUDA version used by PyTorch\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "# To check if CUDA is available in your PyTorch installation\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7517e4-de9f-4888-9869-6057b36f0ed5",
   "metadata": {},
   "source": [
    "# Step 3: Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bd772-c95d-4c61-a0a3-2b86146bc53d",
   "metadata": {},
   "source": [
    "Specifically for the torchvision reference scripts to work, the dataset `__getitem__` should return a tuple `(image, target)`, with:\n",
    "\n",
    "* `image`: a PIL Image of size (H, W)\n",
    "* `target`: a dictionary containing the following fields\n",
    "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
    "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
    "    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n",
    "    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. `visibility=0` means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt `references/detection/transforms.py` for your new keypoint representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bae1d-734b-4b7a-9588-86fc7a75cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms for training batches\n",
    "train_tfm = T.Compose([\n",
    "    T.ToTensor(),  # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    #T.RandomHorizontalFlip(0.5)  # randomly flip the training images\n",
    "])\n",
    "\n",
    "# Define data transforms for validation batches\n",
    "val_tfm = T.ToTensor()\n",
    "\n",
    "# Define datasets\n",
    "dataset = TowerDataset('data/', train_tfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd295922-1819-45e1-bcfb-994451c0e384",
   "metadata": {},
   "source": [
    "# Step 4: Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d837d27-4c0b-4ac8-8f89-501a6e409ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "# Define your custom sizes\n",
    "min_size = 4000  # The minimum size of the image during training/testing\n",
    "max_size = 5300  # The maximum size of the image during training/testing\n",
    "\n",
    "# Adjust the min_size and max_size of the transform\n",
    "#model.transform.min_size = (min_size,)\n",
    "#model.transform.max_size = max_size\n",
    "\n",
    "# Modify the classifier to fit the number of classes\n",
    "num_classes = 5  # Your number of classes + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662cb3c0-b7eb-48f7-b217-5f862aef97d9",
   "metadata": {},
   "source": [
    "# Step 5: Initialize data loaders + train the mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0ad20-575f-4323-a1ee-358b8560888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.detection.utils import collate_fn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "#split the dataset in training and test set\n",
    "number = len(dataset)\n",
    "train_number = math.ceil(number * 0.7)\n",
    "dataset_train, dataset_val = random_split(dataset, [155, 65])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader_train = DataLoader(\n",
    "    dataset_train, batch_size=2, shuffle=True, num_workers=6,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    dataset_val, batch_size=2, shuffle=False, num_workers=6,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d805bd5-dc03-4939-b633-05d729cd11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = StepLR(optimizer,\n",
    "                      step_size=9,\n",
    "                      gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62855b4-4c26-4a85-8c02-6d1e9dbe8a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib.detection.engine import train_one_epoch, evaluate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print('start')\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#from PIL import ImageFile\n",
    "from lib.detection.coco_utils import get_coco_api_from_dataset\n",
    "coco = get_coco_api_from_dataset(data_loader_val.dataset)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train,\n",
    "                    device, epoch, writer=writer)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(f'Epoch {epoch} training done')\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # evaluate on the validation dataset\n",
    "    evaluate(model, data_loader_val, device, epoch, coco, writer=writer)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbbb5a4-77ee-4aed-9404-2f3cc4ede1ff",
   "metadata": {},
   "source": [
    "# Step 6: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4395cbc-511c-4390-91a7-3e9712e9cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_val[1]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb74e48-29d6-422d-84da-9e938f4a607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83adfd3-e296-4bf5-993c-52692ddc0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "im = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for index, box in enumerate(prediction[0]['boxes'].cpu().numpy()):\n",
    "    if prediction[0]['scores'][index] > 0.45:\n",
    "        draw.rectangle(box, width=5, outline=\"red\")\n",
    "        text = str(prediction[0]['labels'][index].item())\n",
    "        text = text + ' score: ' + str(round(prediction[0]['scores'][index].item(),2))\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=30)\n",
    "        text_position = (box[0], box[3])\n",
    "        draw.text(text_position, text, fill=\"red\", font=font)\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdea91c-ef2f-4751-8fa0-9a98dfc9ea50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfc95d-7c83-4a9c-bc58-19d96be3273c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
